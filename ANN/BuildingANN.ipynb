{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Artificial Neural Network\n",
    "based on: Udemy DEEP LEARNING A-Zâ„¢ Course\n",
    "### Setup\n",
    "To setup, we need to have these packages installed: <br>\n",
    "1- Tensorflow<br>\n",
    "2- Keras<br>\n",
    "3- Theano | we will not work with this <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classification problem\n",
    "based on the multi-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Theano is a mathematical library based on numpy, that can run on the GPU\n",
    "#import theano\n",
    "import keras\n",
    "\n",
    "# we will need these for the ANN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# will need this for data pre-processing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# will need this for splitting the data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# will need it for the scaling!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# will need to calculate the loss\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data pre-processing\n",
    "Reading data from the CSV file and make it ready for the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[619 'France' 'Female' 42 2 0.0 1 1 1 101348.88]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "# we only need features from 3 to 12 (upper bound is not inclusive)\n",
    "X = dataset.iloc[:,3:13].values\n",
    "y = dataset.iloc[:,13].values\n",
    "print (X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data\n",
    "There are two variables that are categorial, country and gender <br>\n",
    "So, we convert them into one integer values! So if we apply it to country, it will replcae France, Germany, and Spain with 0,1,2! Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will handle the country\n",
    "labelEncoderX01 = LabelEncoder()\n",
    "X[:,1] = labelEncoderX01.fit_transform(X[:,1])\n",
    "\n",
    "# this will handle the gender\n",
    "labelEncoderX02 = LabelEncoder()\n",
    "X[:,2] = labelEncoderX02.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy coding\n",
    "Now, we need to make some dummy coding! This will prevent the data to make this impression that categoprial values are actually impying any order. Spain is not higher or lower than France nor Germany.\n",
    "We don't ned to do it with gender, since it only has two variables.\n",
    "It basically make that 1,2,3 for countries into 3 different variables:\n",
    "France  0, 0, 1\n",
    "Germany 0, 1, 0\n",
    "Spain   1, 0, 0\n",
    "But we need to remove one of them, because we don't need the third one (if it is not German or French, then it is Spanish!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(categorical_features=[1])\n",
    "X= oneHotEncoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and test\n",
    "Easy peasy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest = train_test_split(X,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "We must normalize the data, otherwise one feature is going to dominate the others! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "# we first need to find the scale\n",
    "XTrain = sc.fit_transform(XTrain)\n",
    "# then we apply the same scale to the test set!\n",
    "XTest = sc.transform(XTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model\n",
    "We will do it by defining a sequence of layers.<br>\n",
    "We mostly use the ReLu activation for the hidden layers and then Sigmoid for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the network\n",
    "classifier = Sequential()\n",
    "\n",
    "# number of nodes in the layer is average of input layers (11 in this case) and output units (1)\n",
    "# or using a fine tuning manually!\n",
    "\n",
    "# adding the input layer and the hidden layer\n",
    "classifier.add(Dense(units= 6, kernel_initializer= \"uniform\",activation = \"relu\",input_dim= 11))\n",
    "\n",
    "# adding the second hidden layer\n",
    "classifier.add(Dense(units= 6, kernel_initializer= \"uniform\",activation = \"relu\"))\n",
    "\n",
    "\n",
    "# adding the output layer\n",
    "classifier.add(Dense(units= 1, kernel_initializer= \"uniform\",activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier.compile(optimizer = 'adam', loss ='binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.4324 - acc: 0.7960\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4321 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4318 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4316 - acc: 0.7960\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4313 - acc: 0.7960\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4310 - acc: 0.7960\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4307 - acc: 0.7960\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4305 - acc: 0.7960\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4303 - acc: 0.7960\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.4301 - acc: 0.7960\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4298 - acc: 0.7960\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4295 - acc: 0.7960\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4292 - acc: 0.7960\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4289 - acc: 0.7960\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4285 - acc: 0.7960\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.4282 - acc: 0.7960\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4278 - acc: 0.7960\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4274 - acc: 0.7960\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4270 - acc: 0.7960\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4266 - acc: 0.7960\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4261 - acc: 0.7960\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4257 - acc: 0.7960\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4253 - acc: 0.7960\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4249 - acc: 0.7960\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4244 - acc: 0.7960\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4240 - acc: 0.7960\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4235 - acc: 0.7960\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.4231 - acc: 0.7960\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.4227 - acc: 0.7960\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.4222 - acc: 0.7960\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4218 - acc: 0.7960\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4213 - acc: 0.7960\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4208 - acc: 0.8070\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4204 - acc: 0.8151\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4200 - acc: 0.8155\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4195 - acc: 0.8177\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4191 - acc: 0.8183\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4187 - acc: 0.8193\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4183 - acc: 0.8207\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4178 - acc: 0.8212\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4173 - acc: 0.8229\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4169 - acc: 0.8234\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4165 - acc: 0.8241\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4160 - acc: 0.8247\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4156 - acc: 0.8244\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4152 - acc: 0.8243\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4147 - acc: 0.8254\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4143 - acc: 0.8266\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4139 - acc: 0.8271\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4135 - acc: 0.8278\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4131 - acc: 0.8276\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4128 - acc: 0.8277\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4124 - acc: 0.8289\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4120 - acc: 0.8293\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4117 - acc: 0.8300\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4114 - acc: 0.8296\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4111 - acc: 0.8295\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.4108 - acc: 0.8298\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.4105 - acc: 0.8300\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.4102 - acc: 0.8298\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.4099 - acc: 0.8299\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4096 - acc: 0.8301\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4093 - acc: 0.8308\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4091 - acc: 0.8304\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4087 - acc: 0.8306\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4085 - acc: 0.8310\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4082 - acc: 0.8302\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4079 - acc: 0.8303\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4076 - acc: 0.8305\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4074 - acc: 0.8311\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4071 - acc: 0.8320\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4068 - acc: 0.8318\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4065 - acc: 0.8320\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4062 - acc: 0.8320\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4060 - acc: 0.8325\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4056 - acc: 0.8328\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4054 - acc: 0.8329\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4051 - acc: 0.8322\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4049 - acc: 0.8321\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4046 - acc: 0.8321\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4044 - acc: 0.8315\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4041 - acc: 0.8318\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4039 - acc: 0.8324\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4037 - acc: 0.8317\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4034 - acc: 0.8314\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4032 - acc: 0.8315\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4030 - acc: 0.8325\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.4027 - acc: 0.8324\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.4025 - acc: 0.8330\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.4022 - acc: 0.8327\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.4020 - acc: 0.8338\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4017 - acc: 0.8338\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4015 - acc: 0.8339\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4013 - acc: 0.8340\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.4011 - acc: 0.8340\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4009 - acc: 0.8342\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.4006 - acc: 0.8351\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.4004 - acc: 0.8347\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4003 - acc: 0.8347\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.4001 - acc: 0.8346\n",
      "Wall time: 6.77 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x185762bfe10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier.fit(XTrain,yTrain, batch_size = 1000, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1537   58]\n",
      " [ 258  147]]\n",
      "accuracy 0.842\n",
      "Wall time: 64.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this is the probability\n",
    "yPred = classifier.predict(XTest)\n",
    "# we need to convert probability to 0 and 1\n",
    "yPred = (yPred > 0.5)\n",
    "cm = confusion_matrix(yTest, yPred )\n",
    "accuracy = (cm[0,0]+cm[1,1]) / np.sum(cm) \n",
    "print(cm)\n",
    "print(\"accuracy {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for a data sample\n",
    "Let's test the data for this sample:\n",
    "France, 600, Male, 40, 3, 60000, 2, Yes, Yes, 50000\n",
    "We use np.array([[...]]) to make a (,11) ndArray, if we use only one bracket, it will be (11,) and will not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "sample = np.array([[0.0, 0.0, 600.0, 1.0, 40.0,3.0,60000.0,2.0,1.0,1.0,50000.0]])\n",
    "sample = sc.transform(sample)\n",
    "samplePred = classifier.predict(sample)\n",
    "samplePred = (samplePred > 0.5)\n",
    "print (samplePred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
